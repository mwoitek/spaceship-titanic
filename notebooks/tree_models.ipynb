{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "135477ff-7a8c-4a09-b473-8407198097de",
   "metadata": {},
   "source": [
    "# Spaceship Titanic: Tree Models\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d071512d-290c-4b91-9758-841830063d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from IPython.display import display\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992722fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2af2498",
   "metadata": {},
   "source": [
    "## Download data (if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25858327",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path.cwd().parent / \"input\" / \"spaceship-titanic\"\n",
    "if not data_dir.exists():\n",
    "    data_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a479d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(base_url: str, file_name: str) -> None:\n",
    "    file_path = data_dir / file_name\n",
    "    if file_path.exists():\n",
    "        print(f\"File {file_name} already exists. Nothing to do.\")\n",
    "        return\n",
    "\n",
    "    url = urljoin(base_url, file_name)\n",
    "    response = requests.get(url, stream=True)  # noqa: S113\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to download file {file_name}\")\n",
    "        return\n",
    "\n",
    "    with file_path.open(\"wb\") as file:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            file.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8658577a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "base_url = \"https://raw.githubusercontent.com/mwoitek/spaceship-titanic/master/input/spaceship-titanic\"\n",
    "download_file(base_url, \"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722710b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "download_file(base_url, \"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd00876b",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2191fa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "df_train = pd.read_csv(data_dir / \"train.csv\")\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca80655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "df_test = pd.read_csv(data_dir / \"test.csv\")\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80afb508",
   "metadata": {},
   "source": [
    "## Create features from `PassengerId`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76277cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group\n",
    "df_train[\"Group\"] = df_train[\"PassengerId\"].str.split(\"_\", expand=True).iloc[:, 0]\n",
    "df_test[\"Group\"] = df_test[\"PassengerId\"].str.split(\"_\", expand=True).iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b03b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupSize\n",
    "df_train = df_train.join(\n",
    "    df_train.groupby(by=\"Group\").agg(GroupSize=pd.NamedAgg(column=\"PassengerId\", aggfunc=\"count\")),\n",
    "    on=\"Group\",\n",
    ")\n",
    "df_test = df_test.join(\n",
    "    df_test.groupby(by=\"Group\").agg(GroupSize=pd.NamedAgg(column=\"PassengerId\", aggfunc=\"count\")),\n",
    "    on=\"Group\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cae6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set indexes\n",
    "df_train = df_train.set_index(\"PassengerId\", verify_integrity=True)\n",
    "df_test = df_test.set_index(\"PassengerId\", verify_integrity=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae624ff8",
   "metadata": {},
   "source": [
    "## New features from `Cabin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a703ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CabinDeck, CabinNum and CabinSide\n",
    "df_train = df_train.join(\n",
    "    df_train[\"Cabin\"]\n",
    "    .str.split(\"/\", expand=True)\n",
    "    .rename(columns={0: \"CabinDeck\", 1: \"CabinNum\", 2: \"CabinSide\"})\n",
    ")\n",
    "df_test = df_test.join(\n",
    "    df_test[\"Cabin\"]\n",
    "    .str.split(\"/\", expand=True)\n",
    "    .rename(columns={0: \"CabinDeck\", 1: \"CabinNum\", 2: \"CabinSide\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2226a389",
   "metadata": {},
   "source": [
    "## Using the `Name` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbace20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Surname column\n",
    "df_train = df_train.assign(Surname=df_train[\"Name\"].str.split(\" \", expand=True).iloc[:, 1])\n",
    "df_test = df_test.assign(Surname=df_test[\"Name\"].str.split(\" \", expand=True).iloc[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd020e9",
   "metadata": {},
   "source": [
    "## Impute some missing values\n",
    "Passengers who belong to the same group also come from the same home planet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53df75c3-56d5-406f-b98d-97917f0dbcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    df_train[df_train[\"HomePlanet\"].notna()]\n",
    "    .groupby(\"Group\")\n",
    "    .agg({\"HomePlanet\": \"nunique\"})\n",
    "    .eq(1)\n",
    "    .all(axis=None)\n",
    ")\n",
    "assert (\n",
    "    df_test[df_test[\"HomePlanet\"].notna()]\n",
    "    .groupby(\"Group\")\n",
    "    .agg({\"HomePlanet\": \"nunique\"})\n",
    "    .eq(1)\n",
    "    .all(axis=None)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c06dfbe",
   "metadata": {},
   "source": [
    "Using group data to impute some missing `HomePlanet` values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24879d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "df_1 = (\n",
    "    df_train.loc[df_train[\"GroupSize\"].gt(1) & df_train[\"HomePlanet\"].notna(), [\"Group\", \"HomePlanet\"]]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "query = \"GroupSize > 1 and Group in @df_1.Group and HomePlanet.isna()\"\n",
    "df_2 = df_train.query(query).loc[:, [\"Group\"]].reset_index()\n",
    "df_3 = df_2.merge(df_1, on=\"Group\").drop(columns=\"Group\").set_index(\"PassengerId\")\n",
    "df_train.loc[df_3.index, \"HomePlanet\"] = df_3[\"HomePlanet\"]\n",
    "del df_1, df_2, df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6e8e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "df_1 = (\n",
    "    df_test.loc[df_test[\"GroupSize\"].gt(1) & df_test[\"HomePlanet\"].notna(), [\"Group\", \"HomePlanet\"]]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "df_2 = df_test.query(query).loc[:, [\"Group\"]].reset_index()\n",
    "df_3 = df_2.merge(df_1, on=\"Group\").drop(columns=\"Group\").set_index(\"PassengerId\")\n",
    "df_test.loc[df_3.index, \"HomePlanet\"] = df_3[\"HomePlanet\"]\n",
    "del df_1, df_2, df_3, query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55abffeb",
   "metadata": {},
   "source": [
    "Passengers that belong to the same group were on the same side of the spaceship:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099982bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    df_train[df_train[\"CabinSide\"].notna()]\n",
    "    .groupby(by=\"Group\")\n",
    "    .agg({\"CabinSide\": \"nunique\"})\n",
    "    .eq(1)\n",
    "    .all(axis=None)\n",
    ")\n",
    "assert (\n",
    "    df_test[df_test[\"CabinSide\"].notna()]\n",
    "    .groupby(by=\"Group\")\n",
    "    .agg({\"CabinSide\": \"nunique\"})\n",
    "    .eq(1)\n",
    "    .all(axis=None)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d52e5a",
   "metadata": {},
   "source": [
    "Fill some missing `CabinSide` values using group data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b04f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "df_1 = (\n",
    "    df_train.query(\"GroupSize > 1 and CabinSide.notna()\")\n",
    "    .groupby(\"Group\")\n",
    "    .agg({\"CabinSide\": \"first\"})\n",
    "    .reset_index()\n",
    ")\n",
    "query = \"GroupSize > 1 and Group in @df_1.Group and CabinSide.isna()\"\n",
    "df_2 = df_train.query(query).loc[:, [\"Group\"]].reset_index()\n",
    "df_3 = df_2.merge(df_1, on=\"Group\").drop(columns=\"Group\").set_index(\"PassengerId\")\n",
    "df_train.loc[df_3.index, \"CabinSide\"] = df_3[\"CabinSide\"]\n",
    "del df_1, df_2, df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62e3017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "df_1 = (\n",
    "    df_test.query(\"GroupSize > 1 and CabinSide.notna()\")\n",
    "    .groupby(\"Group\")\n",
    "    .agg({\"CabinSide\": \"first\"})\n",
    "    .reset_index()\n",
    ")\n",
    "df_2 = df_test.query(query).loc[:, [\"Group\"]].reset_index()\n",
    "df_3 = df_2.merge(df_1, on=\"Group\").drop(columns=\"Group\").set_index(\"PassengerId\")\n",
    "df_test.loc[df_3.index, \"CabinSide\"] = df_3[\"CabinSide\"]\n",
    "del df_1, df_2, df_3, query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dbc83a",
   "metadata": {},
   "source": [
    "Passengers with the same surname are from the same planet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66190580",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    df_train[[\"Surname\", \"HomePlanet\"]]\n",
    "    .dropna()\n",
    "    .groupby(\"Surname\")\n",
    "    .agg({\"HomePlanet\": \"nunique\"})\n",
    "    .eq(1)\n",
    "    .all(axis=None)\n",
    ")\n",
    "assert (\n",
    "    df_test[[\"Surname\", \"HomePlanet\"]]\n",
    "    .dropna()\n",
    "    .groupby(\"Surname\")\n",
    "    .agg({\"HomePlanet\": \"nunique\"})\n",
    "    .eq(1)\n",
    "    .all(axis=None)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dd7531",
   "metadata": {},
   "source": [
    "Use `Surname` to fill more missing `HomePlanet` values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a087fe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "df_sur_1 = (\n",
    "    df_train[[\"Surname\", \"HomePlanet\"]].dropna().groupby(\"Surname\").agg({\"HomePlanet\": \"first\"}).reset_index()\n",
    ")\n",
    "query = \"Surname.notna() and Surname in @df_sur_1.Surname and HomePlanet.isna()\"\n",
    "df_1 = df_train.query(query).loc[:, [\"Surname\"]].reset_index()\n",
    "df_2 = df_1.merge(df_sur_1, on=\"Surname\").drop(columns=\"Surname\").set_index(\"PassengerId\")\n",
    "df_train.loc[df_2.index, \"HomePlanet\"] = df_2[\"HomePlanet\"]\n",
    "del df_1, df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a77322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "\n",
    "# To fix test data, I'll also use some training data. Combine all relevant data:\n",
    "df_sur_2 = (\n",
    "    df_test[[\"Surname\", \"HomePlanet\"]].dropna().groupby(\"Surname\").agg({\"HomePlanet\": \"first\"}).reset_index()\n",
    ")\n",
    "df_sur = pd.concat([df_sur_1, df_sur_2.query(\"Surname not in @df_sur_1.Surname\")], ignore_index=True)\n",
    "del df_sur_1, df_sur_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9179e1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = query.replace(\"df_sur_1\", \"df_sur\")\n",
    "df_1 = df_test.query(query).loc[:, [\"Surname\"]].reset_index()\n",
    "df_2 = df_1.merge(df_sur, on=\"Surname\").drop(columns=\"Surname\").set_index(\"PassengerId\")\n",
    "df_test.loc[df_2.index, \"HomePlanet\"] = df_2[\"HomePlanet\"]\n",
    "del df_1, df_2, df_sur, query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd05be18",
   "metadata": {},
   "source": [
    "No VIP passenger is from Earth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5a9e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"VIP.notna() and VIP == True and HomePlanet.notna()\"\n",
    "assert df_train.query(query).HomePlanet.ne(\"Earth\").all()\n",
    "assert df_test.query(query).HomePlanet.ne(\"Earth\").all()\n",
    "del query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588f3b4b",
   "metadata": {},
   "source": [
    "Impute some missing values of `VIP`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375ab524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "query = \"VIP.isna() and HomePlanet.notna() and HomePlanet == 'Earth'\"\n",
    "idx = df_train.query(query).index\n",
    "df_train.loc[idx, \"VIP\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca684a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "idx = df_test.query(query).index\n",
    "df_test.loc[idx, \"VIP\"] = False\n",
    "del idx, query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502d61ad",
   "metadata": {},
   "source": [
    "Dealing with the \"money columns\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa5b34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All medians equal zero\n",
    "money_cols = [\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\n",
    "assert df_train[money_cols].median().eq(0.0).all()\n",
    "assert df_test[money_cols].median().eq(0.0).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bb3db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with zeros (medians)\n",
    "df_train.loc[:, money_cols] = df_train[money_cols].fillna(0.0)\n",
    "df_test.loc[:, money_cols] = df_test[money_cols].fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85680fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add `TotalSpent` column\n",
    "df_train[\"TotalSpent\"] = df_train[money_cols].agg(\"sum\", axis=1)\n",
    "df_test[\"TotalSpent\"] = df_test[money_cols].agg(\"sum\", axis=1)\n",
    "del money_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac42560",
   "metadata": {},
   "source": [
    "Passengers who spent money were NOT in cryo sleep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec84004",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not df_train.query(\"TotalSpent > 0 and CryoSleep.notna()\").CryoSleep.any()\n",
    "assert not df_test.query(\"TotalSpent > 0 and CryoSleep.notna()\").CryoSleep.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45752f6",
   "metadata": {},
   "source": [
    "Fill some missing `CryoSleep` values based on `TotalSpent`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee9dc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[df_train[\"CryoSleep\"].isna() & df_train[\"TotalSpent\"].gt(0.0), \"CryoSleep\"] = False\n",
    "df_test.loc[df_test[\"CryoSleep\"].isna() & df_test[\"TotalSpent\"].gt(0.0), \"CryoSleep\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2881c0f5",
   "metadata": {},
   "source": [
    "## Missing values that remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c74123",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = [\"HomePlanet\", \"CryoSleep\", \"Destination\", \"Age\", \"VIP\", \"CabinDeck\", \"CabinSide\"]\n",
    "df_miss = df_train.isna().sum().rename(\"Number\").to_frame().rename_axis(\"Feature\", axis=0)\n",
    "df_miss = df_miss[df_miss[\"Number\"] > 0].loc[feats, :]\n",
    "df_miss = df_miss.assign(Percentage=(100.0 * df_miss[\"Number\"] / df_train.shape[0]).round(2)).sort_values(\n",
    "    by=\"Percentage\", ascending=False\n",
    ")\n",
    "df_miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6392d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_miss = df_test.isna().sum().rename(\"Number\").to_frame().rename_axis(\"Feature\", axis=0)\n",
    "df_miss = df_miss[df_miss[\"Number\"] > 0].loc[feats, :]\n",
    "df_miss = df_miss.assign(Percentage=(100.0 * df_miss[\"Number\"] / df_test.shape[0]).round(2)).sort_values(\n",
    "    by=\"Percentage\", ascending=False\n",
    ")\n",
    "df_miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6f0718",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_miss, feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235c4359",
   "metadata": {},
   "source": [
    "## Encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54939af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feats = [\"HomePlanet\", \"CabinDeck\", \"CabinSide\", \"Destination\"]\n",
    "enc = OrdinalEncoder().fit(df_train[cat_feats])\n",
    "df_train.loc[:, cat_feats] = enc.transform(df_train[cat_feats])\n",
    "df_test.loc[:, cat_feats] = enc.transform(df_test[cat_feats])\n",
    "del cat_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5f9cfe",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a3ccb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\n",
    "    \"GroupSize\",\n",
    "    \"HomePlanet\",\n",
    "    \"CryoSleep\",\n",
    "    \"CabinDeck\",\n",
    "    \"CabinSide\",\n",
    "    \"Destination\",\n",
    "    \"Age\",\n",
    "    \"VIP\",\n",
    "    \"RoomService\",\n",
    "    \"FoodCourt\",\n",
    "    \"ShoppingMall\",\n",
    "    \"Spa\",\n",
    "    \"VRDeck\",\n",
    "    \"TotalSpent\",\n",
    "]\n",
    "X = df_train[feature_names]\n",
    "y = df_train[\"Transported\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976314b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = len(feature_names)\n",
    "score_func = lambda X, y: mutual_info_classif(X, y, random_state=0)\n",
    "feature_sets = []\n",
    "\n",
    "for num_features in range(1, max_features + 1):\n",
    "    idx_1 = X[X.notna().all(axis=1)].index\n",
    "    selector = SelectKBest(score_func=score_func, k=num_features).fit(X.loc[idx_1, :], y.loc[idx_1])\n",
    "\n",
    "    idx_2 = selector.get_support(indices=True)\n",
    "    feature_set = selector.feature_names_in_[idx_2].tolist()\n",
    "    feature_sets.append(feature_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3799cac9",
   "metadata": {},
   "source": [
    "## Tree models\n",
    "### Random Forest\n",
    "Accuracy as a function of the number of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94842cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "cv = StratifiedKFold(shuffle=True, random_state=0)\n",
    "rf_params = {\n",
    "    \"n_estimators\": 200,\n",
    "    \"criterion\": \"entropy\",\n",
    "    \"class_weight\": \"balanced_subsample\",\n",
    "    \"random_state\": 0,\n",
    "}\n",
    "\n",
    "for feature_set in feature_sets:\n",
    "    cv_accs = []\n",
    "    X_new = X[feature_set]\n",
    "\n",
    "    for train_idx, test_idx in cv.split(X_new, y):\n",
    "        X_train, X_test = X_new.iloc[train_idx, :], X_new.iloc[test_idx, :]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        rf = RandomForestClassifier(**rf_params).fit(X_train, y_train)\n",
    "        acc = rf.score(X_test, y_test)\n",
    "        cv_accs.append(acc)\n",
    "\n",
    "    mean_acc = np.mean(cv_accs)\n",
    "    accs.append(mean_acc)\n",
    "\n",
    "df_accs = pd.DataFrame(\n",
    "    data={\n",
    "        \"NumFeatures\": np.arange(1, max_features + 1),\n",
    "        \"Accuracy\": accs,\n",
    "        \"FeatureSet\": feature_sets,\n",
    "    }\n",
    ").set_index(\"NumFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b228018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_max = int(df_accs[\"Accuracy\"].idxmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c068bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9.0, 6.0), layout=\"tight\")\n",
    "ax = fig.add_subplot()\n",
    "colors = [\"#2f4f4f\"] * df_accs.shape[0]\n",
    "colors[idx_max - 1] = \"#6039b2\"\n",
    "ax.bar(df_accs.index, df_accs[\"Accuracy\"], color=colors)\n",
    "ax.bar_label(ax.containers[0], fmt=\"%.3f\")  # pyright: ignore [reportArgumentType]\n",
    "ax.set_xticks(df_accs.index)\n",
    "ax.set_xlabel(\"Number of features\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"Accuracy as a function of the number of features\")\n",
    "plt.show()\n",
    "\n",
    "with pd.option_context(\"display.max_colwidth\", None):\n",
    "    display(df_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c44153",
   "metadata": {},
   "source": [
    "Find optimal model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9affcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = df_accs.loc[idx_max, \"FeatureSet\"]\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffd43ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train[feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2334c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(**rf_params)\n",
    "param_grid = {\n",
    "    \"max_depth\": [8, 11, 12, 13, 14],\n",
    "    \"min_samples_split\": [13, 17, 18, 19, 20],\n",
    "    \"min_samples_leaf\": [5, 6, 7, 14, 20],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d001d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(rf, param_grid=param_grid, scoring=\"accuracy\", n_jobs=3).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269643df",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = (\n",
    "    pd.DataFrame(grid_search.cv_results_)\n",
    "    .drop(columns=[\"mean_fit_time\", \"std_fit_time\", \"mean_score_time\", \"std_score_time\", \"params\"])\n",
    "    .set_index(\"rank_test_score\")\n",
    "    .sort_index()\n",
    ")\n",
    "cv_results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd92886",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f981bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
